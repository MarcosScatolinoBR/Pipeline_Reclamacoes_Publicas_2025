# pipeline_reclamacoes_publicas_mar_2025.py
# Autor: Marcos Vinicius do Couto Scatolino
# Projeto: An√°lise Estrat√©gica de Reclama√ß√µes P√∫blicas no setor de Log√≠stica e E-commerce

# ==========================================
# IMPORTA√á√ïES E CONFIGURA√á√ïES INICIAIS
# ==========================================

# Bibliotecas essenciais para manipula√ß√£o de dados, visualiza√ß√£o, NLP e machine learning

import os
import re
from collections import Counter
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from fpdf import FPDF
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix


# ==========================================
# BLOCO 1 ‚Äì Leitura e Limpeza dos Dados
# ==========================================

# Definir o caminho do arquivo
pasta = 'dados'
nome_arquivo = 'base_completa_2025-03.csv'
caminho_dados = os.path.join(pasta, nome_arquivo)

# Verifica se o arquivo existe
if not os.path.exists(caminho_dados):
    raise FileNotFoundError(f'Arquivo n√£o encontrado: {caminho_dados}')

# Carrega o CSV com codifica√ß√£o original
df = pd.read_csv(caminho_dados, sep=';', encoding='latin-1')

# Fun√ß√£o para limpar e padronizar os nomes das colunas
def limpar_colunas(col):
    col = col.encode('latin1').decode('utf-8')
    col = col.replace(' ', '_')
    col = col.replace('√£', 'a').replace('√°', 'a').replace('√¢', 'a').replace('√™', 'e')
    col = col.replace('√©', 'e').replace('√≠', 'i').replace('√≥', 'o').replace('√¥', 'o')
    col = col.replace('√ß', 'c').replace('√∫', 'u').replace('√É', 'A').replace('√ï', 'O')
    col = col.replace('√ä', 'E').replace('√Å', 'A').replace('√â', 'E')
    col = col.lower()
    return col

# Aplica a limpeza nos nomes das colunas
df.columns = [limpar_colunas(c) for c in df.columns]
df.rename(columns={df.columns[0]: 'gestor'}, inplace=True)

# üîß LIMPEZA EXTRA: corrigir acentua√ß√£o nos conte√∫dos das c√©lulas (valores)
for coluna in df.select_dtypes(include='object').columns:
    df[coluna] = df[coluna].apply(lambda x: x.encode('latin1').decode('utf-8') if isinstance(x, str) else x)

# ===============================
# BLOCO 2 ‚Äì Foco: log√≠stica e e-commerce
# ===============================

# 1. Empresas com mais reclama√ß√µes
print('\nüìå Empresas com mais reclama√ß√µes:')
print(df['nome_fantasia'].value_counts().head(20))

# 2. Segmentos de mercado dispon√≠veis
print('\nüìå Segmentos de mercado presentes:')
print(df['segmento_de_mercado'].value_counts().head(20))

# 3. Filtro de empresas de log√≠stica/e-commerce
palavras_chave = [
    'logistica', 'entrega', 'e-commerce', 'transportadora',
    'correios', 'mercado livre', 'magalu', 'amazon'
]

filtro = df['segmento_de_mercado'].str.lower().str.contains('|'.join(palavras_chave), na=False) | \
         df['nome_fantasia'].str.lower().str.contains('|'.join(palavras_chave), na=False)

df_log = df[filtro].copy()
print(f"\nüîç Total de registros filtrados (log√≠stica/e-commerce): {len(df_log)}")

# 4. Principais problemas relatados
print('\nüìå Principais problemas relatados nas empresas filtradas:')
print(df_log['problema'].value_counts().head(15))

# 5. Nota m√©dia x tempo de resposta
df_avaliados = df_log.dropna(subset=['tempo_resposta', 'nota_do_consumidor'])

media_resposta_nota = df_avaliados.groupby('nome_fantasia')[
    ['tempo_resposta', 'nota_do_consumidor']
].mean().sort_values(by='nota_do_consumidor', ascending=False)

print('\nüìä Empresas com melhor m√©dia de nota (avaliadas):')
print(media_resposta_nota.head(10).round(2))

print('\nüìâ Empresas com pior m√©dia de nota (avaliadas):')
print(media_resposta_nota.tail(10).round(2))

# ===============================
# BLOCO 3 ‚Äì Visualiza√ß√µes dos dados
# ===============================

# Estilo atualizado
sns.set_theme(style='whitegrid')
plt.rcParams['axes.unicode_minus'] = False

# ----------------------------
# 1. Gr√°fico de Barras ‚Äì Top 10 Problemas
# ----------------------------
top_problemas = df_log['problema'].value_counts().head(10)

plt.figure(num='Top 10 Problemas Mais Comuns', figsize=(12,6))
sns.barplot(x=top_problemas.values, y=top_problemas.index, palette='crest')
plt.title('Top 10 Problemas Mais Comuns em Log√≠stica/E-commerce')
plt.xlabel('N√∫mero de Reclama√ß√µes')
plt.ylabel('Problema')
plt.tight_layout()
plt.show()

# ----------------------------
# 2. Scatterplot ‚Äì Nota x Tempo de Resposta
# ----------------------------
df_avaliados_plot = df_log.dropna(subset=['tempo_resposta', 'nota_do_consumidor']).copy()

unique_empresas = df_avaliados_plot['nome_fantasia'].unique()
palette = sns.color_palette('tab10', n_colors=len(unique_empresas))
empresa_cor = {nome: palette[i % len(palette)] for i, nome in enumerate(unique_empresas)}

plt.figure(num='Nota x Tempo de Resposta', figsize=(10,6))
sns.scatterplot(
    data=df_avaliados_plot,
    x='tempo_resposta',
    y='nota_do_consumidor',
    hue='nome_fantasia',
    palette=empresa_cor,
    alpha=0.75,
    edgecolor='black'
)
plt.title('Tempo de Resposta x Nota do Consumidor')
plt.xlabel('Tempo de Resposta (dias)')
plt.ylabel('Nota do Consumidor (1 a 5)')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Empresa')
plt.tight_layout()
plt.show()

# ----------------------------
# 3. WordCloud ‚Äì Visual + Frequ√™ncia
# ----------------------------

# Texto base + limpeza
texto_problemas = ' '.join(df_log['problema'].dropna().tolist())
texto_problemas = (
    texto_problemas.replace("NAO", "n√£o")
                   .replace("Nao", "n√£o")
                   .replace("n√É¬£o", "n√£o")
                   .replace("nAo", "n√£o")
                   .lower()
)
texto_problemas = re.sub(r'[^\w\s]', '', texto_problemas)

# Quebra palavras + remo√ß√£o de stopwords
palavras = texto_problemas.split()
stopwords = ['de', 'o', 'a', 'e', 'com', 'para', 'por', 'em', 'do', 'na', 'no', 'da',
             'ao', 'os', 'as', 'dos', 'das', 'se']
palavras_filtradas = [p for p in palavras if p not in stopwords and len(p) > 1]

# Frequ√™ncia e preview
frequencia_palavras = Counter(palavras_filtradas)
print('\nüî† Top 50 palavras mais citadas:')
for palavra, freq in frequencia_palavras.most_common(50):
    print(f'{palavra}: {freq}')

# WordCloud com nome personalizado na janela
texto_filtrado = ' '.join(palavras_filtradas)
wordcloud = WordCloud(
    width=1000,
    height=500,
    background_color='white',
    colormap='Dark2',
    max_words=100
).generate(texto_filtrado)

plt.figure(num='WordCloud ‚Äì Problemas Mais Citados', figsize=(14, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Palavras Mais Citadas nos Problemas Relatados')
plt.tight_layout()
plt.show()

################################
# Clusterniza√ß√£o
################################

# 1. Coleta dos textos dos problemas
problemas = df_log['problema'].dropna().values

# 2. Lista de stopwords personalizadas (em portugu√™s)
stopwords_pt = [
    'de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', 'com', 'n√£o',
    'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas',
    'foi', 'ao', '√†', 'pelo', 'pela', 'sobre', 'entre', 'sem', 'tamb√©m',
    'j√°', 'tem', 'h√°', '√©', 'ser', 'est√°', 'estavam'
]

# 3. Vetoriza√ß√£o com TF-IDF
vectorizer = TfidfVectorizer(stop_words=stopwords_pt, max_features=1000)
X = vectorizer.fit_transform(problemas)

# 4. Clusteriza√ß√£o com KMeans
k = 5
modelo_kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
modelo_kmeans.fit(X)

# 5. Redu√ß√£o dimensional com PCA para visualiza√ß√£o
pca = PCA(n_components=2, random_state=42)
X_reduzido = pca.fit_transform(X.toarray())

# 6. Visualiza√ß√£o dos clusters (com legenda)
plt.figure(num='Clusteriza√ß√£o de Reclama√ß√µes com KMeans', figsize=(12, 6))
cores = plt.cm.tab10(np.arange(k))

for i in range(k):
    plt.scatter(
        X_reduzido[modelo_kmeans.labels_ == i, 0],
        X_reduzido[modelo_kmeans.labels_ == i, 1],
        color=cores[i],
        label=f'Cluster {i}',
        alpha=0.7
    )

plt.title('Agrupamento de Reclama√ß√µes com KMeans (PCA reduzido)')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.legend(title='Clusters', loc='best')
plt.grid(True)
plt.tight_layout()
plt.show()

# 7. Adiciona os clusters ao DataFrame original
df_log['cluster'] = modelo_kmeans.labels_

# 8. Mostra exemplos reais de cada cluster
print('\nüìå Exemplos de reclama√ß√µes por cluster:\n')
for i in range(k):
    print(f'\nüß© Cluster {i}:')
    print(df_log[df_log['cluster'] == i]['problema'].sample(5, random_state=42).to_list())


####
# classifica√ß√£o dos clusters com regress√£o log√≠stica
###

# 1. Dados de entrada: texto dos problemas
problemas = df_log['problema'].dropna()
clusters = df_log.loc[problemas.index, 'cluster']

# 2. Vetoriza√ß√£o com o mesmo TF-IDF (reaproveitar ou refazer)
vectorizer = TfidfVectorizer(stop_words=stopwords_pt, max_features=1000)
X = vectorizer.fit_transform(problemas)
y = clusters

# 3. Separar em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# 4. Treinar o modelo
modelo_lr = LogisticRegression(max_iter=1000)
modelo_lr.fit(X_train, y_train)

# 5. Avaliar a performance
y_pred = modelo_lr.predict(X_test)

print("\nüìä Classifica√ß√£o ‚Äì Regress√£o Log√≠stica sobre Clusters")
print(classification_report(y_test, y_pred))
print("üìâ Matriz de Confus√£o:")
print(confusion_matrix(y_test, y_pred))

# 6. Prever a qual cluster pertence uma nova reclama√ß√£o
nova_reclamacao = ["O produto chegou danificado e a empresa n√£o responde"]
nova_vetor = vectorizer.transform(nova_reclamacao)
cluster_previsto = modelo_lr.predict(nova_vetor)[0]

print(f"\nüß† Nova reclama√ß√£o: \"{nova_reclamacao[0]}\"")
print(f"‚û°Ô∏è Classificada no cluster {cluster_previsto}")

###
# exporta√ß√£o de graficos
###

# Criar pasta de sa√≠da, se n√£o existir
os.makedirs('outputs', exist_ok=True)

# 1. Gr√°fico de barras ‚Äì Top 10 Problemas
plt.figure(figsize=(12,6))
sns.barplot(x=top_problemas.values, y=top_problemas.index, palette='crest')
plt.title('Top 10 Problemas Mais Comuns em Log√≠stica/E-commerce')
plt.xlabel('N√∫mero de Reclama√ß√µes')
plt.ylabel('Problema')
plt.tight_layout()
plt.savefig('outputs/top_problemas.png', dpi=300)
plt.close()

# 2. Gr√°fico de dispers√£o ‚Äì Tempo de resposta x Nota
plt.figure(figsize=(10,6))
for i in range(k):
    plt.scatter(
        X_reduzido[modelo_kmeans.labels_ == i, 0],
        X_reduzido[modelo_kmeans.labels_ == i, 1],
        label=f'Cluster {i}',
        alpha=0.7
    )
plt.title('Clusteriza√ß√£o de Reclama√ß√µes com KMeans (PCA)')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig('outputs/clusters_kmeans.png', dpi=300)
plt.close()

# 3. Gr√°fico de dispers√£o ‚Äì Nota x Tempo de Resposta (com legenda)
plt.figure(figsize=(10,6))
sns.scatterplot(
    data=df_avaliados_plot,
    x='tempo_resposta',
    y='nota_do_consumidor',
    hue='nome_fantasia',
    palette='tab10',
    alpha=0.75,
    edgecolor='black'
)
plt.title('Tempo de Resposta x Nota do Consumidor')
plt.xlabel('Tempo de Resposta (dias)')
plt.ylabel('Nota do Consumidor (1 a 5)')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Empresa')
plt.tight_layout()
plt.savefig('outputs/nota_vs_resposta.png', dpi=300)
plt.close()

# 4. Wordcloud ‚Äì Palavras mais citadas
wordcloud = WordCloud(
    width=1000,
    height=500,
    background_color='white',
    colormap='Dark2',
    max_words=100
).generate(texto_filtrado)

plt.figure(figsize=(14, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Palavras Mais Citadas nos Problemas Relatados')
plt.tight_layout()
plt.savefig('outputs/wordcloud_problemas.png', dpi=300)
plt.close()

class PDF(FPDF):
    def header(self):
        pass  # sem cabe√ßalho autom√°tico

    def footer(self):
        self.set_y(-15)
        self.set_font('Arial', 'I', 8)
        self.cell(0, 10, f'P√°gina {self.page_no()}', align='C')

    def add_section(self, titulo, imagem=None, texto=None):
        # Adiciona quebra de p√°gina se n√£o tiver espa√ßo para t√≠tulo + imagem
        altura_prevista = 10 + (90 if imagem else 0) + (30 if texto else 0)
        if self.get_y() + altura_prevista > self.h - 20:
            self.add_page()

        self.set_font("Arial", 'B', 12)
        self.cell(0, 10, titulo, ln=True)

        if imagem and os.path.exists(imagem):
            self.image(imagem, w=170)
            self.ln(5)

        if texto:
            self.set_font("Arial", '', 11)
            self.multi_cell(0, 8, texto, align='J')
            self.ln(5)

###
# PDF
###

def limpar_texto(texto):
    return (
        texto.replace("‚Äô", "'")
             .replace("‚Äì", "-")
             .replace("‚Äî", "-")
             .replace("‚Äú", '"')
             .replace("‚Äù", '"')
             .replace("‚Ä¢", "-")
             .replace("‚Ä¶", "...")
             .encode("latin-1", errors="ignore")
             .decode("latin-1")
    )

class PDF_ABNT(FPDF):
    def __init__(self):
        super().__init__('P', 'mm', 'A4')
        self.set_auto_page_break(auto=True, margin=20)
        self.set_margins(left=30, top=30, right=20)

    def footer(self):
        self.set_y(-15)
        self.set_font('Arial', 'I', 9)
        self.cell(0, 10, f'P√°gina {self.page_no()}', align='C')

    def add_section(self, titulo, imagem=None, texto=None):
        altura_prevista = 10 + (90 if imagem else 0) + (60 if texto else 0)
        if self.get_y() + altura_prevista > self.h - 20:
            self.add_page()

        self.set_font("Arial", 'B', 12)
        self.multi_cell(0, 10, limpar_texto(titulo))
        self.ln(1)

        if imagem and os.path.exists(imagem):
            largura_disponivel = self.w - self.l_margin - self.r_margin
            self.image(imagem, w=largura_disponivel)
            self.ln(5)

        if texto:
            self.set_font("Arial", '', 11)
            self.multi_cell(0, 8, limpar_texto(texto), align='J')
            self.ln(5)

# Criar pasta de sa√≠da
os.makedirs("outputs", exist_ok=True)

# Criar documento PDF com padr√£o ABNT
pdf = PDF_ABNT()
pdf.add_page()

# Capa
pdf.set_font("Arial", 'B', 14)
pdf.multi_cell(0, 10, limpar_texto("An√°lise Estrat√©gica de Reclama√ß√µes P√∫blicas ‚Äì Setor de Log√≠stica e E-commerce (Mar/2025)"))
pdf.set_font("Arial", '', 11)
pdf.cell(0, 10, limpar_texto("Por: Marcos Vinicius do Couto Scatolino | Analista de Dados"), ln=True)
pdf.ln(10)

# Introdu√ß√£o
intro = (
    "Este relat√≥rio anal√≠tico apresenta os principais pontos de fric√ß√£o no relacionamento entre consumidores e empresas do setor de "
    "log√≠stica e e-commerce, a partir de um banco p√∫blico de mais de 150 mil registros de reclama√ß√µes at√© mar√ßo de 2025. "
    "Atrav√©s de t√©cnicas de an√°lise de dados, visualiza√ß√£o e machine learning, foi poss√≠vel extrair padr√µes valiosos que podem ajudar empresas "
    "a reduzir atritos, elevar sua reputa√ß√£o, otimizar o tempo de resposta e entregar uma experi√™ncia de cliente mais inteligente, humana e eficiente."
)
pdf.set_font("Arial", '', 11)
pdf.multi_cell(0, 8, limpar_texto(intro), align='J')
pdf.ln(10)

# Se√ß√µes com gr√°ficos e an√°lise interpretativa
pdf.add_section(
    "1. Principais Problemas Apontados por Clientes",
    imagem="outputs/top_problemas.png",
    texto=(
        "O gr√°fico revela que 'N√£o entrega' e 'Demora na entrega' s√£o os maiores motivos de insatisfa√ß√£o, seguidos por reembolso dif√≠cil e venda enganosa. "
        "A recorr√™ncia desses temas indica falhas estruturais na jornada log√≠stica e no p√≥s-venda. Empresas que buscam reduzir o CAC e ampliar o LTV "
        "devem focar em resolver esses gargalos com prioridade estrat√©gica, integrando dados de SAC, log√≠stica e atendimento em tempo real."
    )
)

pdf.add_section(
    "2. Correla√ß√£o: Tempo de Resposta e Nota do Consumidor",
    imagem="outputs/nota_vs_resposta.png",
    texto=(
        "Empresas que respondem mais rapidamente √†s reclama√ß√µes tendem a ter melhores avalia√ß√µes. Isso demonstra que tempo de resposta √© um KPI cr√≠tico "
        "n√£o apenas operacional, mas de reputa√ß√£o. O dado refor√ßa a import√¢ncia de um SAC orientado por dados (SAC 4.0) e fluxos automatizados com supervis√£o humana, "
        "otimizando o relacionamento e antecipando crises."
    )
)

pdf.add_section(
    "3. Padr√µes Ocultos: Segmenta√ß√£o Autom√°tica via KMeans",
    imagem="outputs/clusters_kmeans.png",
    texto=(
        "Utilizando KMeans, foram agrupadas reclama√ß√µes por similaridade sem r√≥tulo pr√©vio. Isso permite categorizar de forma inteligente grandes volumes de feedback, "
        "facilitando a prioriza√ß√£o de melhorias. Empresas podem usar essa l√≥gica para criar etiquetas autom√°ticas e dashboards gerenciais mais inteligentes, otimizando esfor√ßos de UX, log√≠stica e suporte."
    )
)

pdf.add_section(
    "4. An√°lise de Linguagem Natural: Palavras Mais Citadas",
    imagem="outputs/wordcloud_problemas.png",
    texto=(
        "Termos como 'entrega', 'produto', 'reembolso', 'nao' (normalizado) e 'cancelamento' dominam a percep√ß√£o dos clientes. Esses pontos evidenciam que, para al√©m de falhas operacionais, "
        "h√° um desalinhamento entre promessa e entrega, impactando diretamente a confian√ßa. A an√°lise textual refor√ßa a urg√™ncia de comunica√ß√£o transparente, fluxos claros e escuta ativa."
    )
)

pdf.add_section(
    "5. Conclus√µes Estrat√©gicas e Recomenda√ß√µes",
    texto=(
        "Empresas que atuam em setores intensivos em log√≠stica e relacionamento precisam tratar dados de SAC como fonte prim√°ria de intelig√™ncia. "
        "Os insights apresentados aqui mostram que a experi√™ncia do cliente come√ßa muito antes da entrega, e se prolonga no p√≥s-venda. "
        "Organiza√ß√µes que integram dados, escutam com velocidade e reagem com empatia se posicionam √† frente da concorr√™ncia. "
        "Recomenda-se implementar sistemas de triagem autom√°tica com base em clusteriza√ß√£o, prioriza√ß√£o por sentimento e automatiza√ß√£o de contato inicial, com supervis√£o anal√≠tica cont√≠nua."
    )
)

# Salvar PDF
pdf.output("outputs/relatorio_final_abnt.pdf")
